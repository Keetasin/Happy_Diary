{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4Wbfs-cUscU"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1740401926096,
     "user": {
      "displayName": "Keetasin Kongsee",
      "userId": "16813886729039888270"
     },
     "user_tz": -420
    },
    "id": "f7R2QQNDW7FQ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DbLvQzbmJBjV"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_dataset_augmented11.csv')\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sk7zNwrZARCI"
   },
   "outputs": [],
   "source": [
    "# ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "df = pd.read_csv('/content/cleaned_dataset_augmented11.csv')\n",
    "\n",
    "# Tokenization & Padding\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = 50\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encode labels\n",
    "label_order = ['minimum', 'mild', 'moderate', 'severe']\n",
    "labels = pd.Categorical(df['label'], categories=label_order, ordered=True)\n",
    "labels = pd.get_dummies(labels).values\n",
    "\n",
    "# ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• Train/Test\n",
    "x_train, x_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• LSTM\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),\n",
    "    SpatialDropout1D(0.3),  # ‡∏•‡∏î Overfitting\n",
    "    Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=l1_l2(0.0005, 0.001))),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    LSTM(32, return_sequences=False, kernel_regularizer=l1_l2(0.0005, 0.001)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(24, activation='relu', kernel_regularizer=l1_l2(0.0005, 0.002)),\n",
    "    Dropout(0.5),\n",
    "    Dense(labels.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile ‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "optimizer = Adam(learning_rate=0.0005, clipnorm=1.0)  # Gradient Clipping\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-5, verbose=1)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True, verbose=1)\n",
    "\n",
    "# ‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=50, batch_size=32,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=label_order))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 144,
     "status": "ok",
     "timestamp": 1740401805777,
     "user": {
      "displayName": "Keetasin Kongsee",
      "userId": "16813886729039888270"
     },
     "user_tz": -420
    },
    "id": "_EcePKssXwHR",
    "outputId": "b9d8ab72-f67b-4247-86e6-7feb46e5e00f"
   },
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1740401805781,
     "user": {
      "displayName": "Keetasin Kongsee",
      "userId": "16813886729039888270"
     },
     "user_tz": -420
    },
    "id": "2St9wko8aVee"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 924,
     "status": "ok",
     "timestamp": 1740402749541,
     "user": {
      "displayName": "Keetasin Kongsee",
      "userId": "16813886729039888270"
     },
     "user_tz": -420
    },
    "id": "j9WNrP2xFmQ-",
    "outputId": "00416b36-adf0-47ea-f285-59eaa725e33e"
   },
   "outputs": [],
   "source": [
    "# ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Confusion Matrix\n",
    "cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô Heatmap\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=label_order, yticklabels=label_order)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ane8VBs__Grj"
   },
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á DataFrame ‡∏Ç‡∏≠‡∏á‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö (‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏°‡∏≤‡πÅ‡∏™‡∏î‡∏á)\n",
    "df_test = df.iloc[x_test.shape[0]*-1:].copy()\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_test['predicted_label'] = [label_order[i] for i in y_pred_classes]\n",
    "\n",
    "# ‡πÅ‡∏™‡∏î‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Ñ‡∏•‡∏≤‡∏™\n",
    "for label in label_order:\n",
    "    print(f\"\\nüîπ ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô '{label}':\")\n",
    "    examples = df_test[df_test['predicted_label'] == label]['text'].head(5).tolist()\n",
    "    for i, text in enumerate(examples, 1):\n",
    "        print(f\"{i}. {text}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPAuTRT0eLZxYyxK3aieznV",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1B-kEo-KNcD9W4fVws0KAWj34tJkvOsUn",
     "timestamp": 1740302860952
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
